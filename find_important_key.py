from pandas import read_excel, DataFrame
from numpy import array, asarray
from sklearn.feature_extraction.text import TfidfVectorizer
from hazm import Normalizer, word_tokenize, stopwords_list, POSTagger, Lemmatizer
import os

from utility import DATA_PATH


class KeyWordExtraction:
    FILE_NAME = "comments.xlsx"

    def __init__(self):
        self.df = DataFrame()
        self.normalizer = Normalizer()
        self.stopwords = set(stopwords_list())
        self.tagger = POSTagger(model=os.path.join(DATA_PATH, "resources/pos_tagger.model"))
        self.lemmatizer = Lemmatizer()
        self.tokenized = []
        self.feature_names = array([])
        self.tfidf_matrix = None

    def read_data(self):
        self.df = read_excel(os.path.join(DATA_PATH, self.FILE_NAME))
        # ÙÙ‚Ø· Ø³ØªÙˆÙ† comment Ø±Ùˆ Ù†Ú¯Ù‡â€ŒÙ…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ… Ùˆ Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø±Ùˆ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        self.df = self.df[["comment"]].dropna().reset_index(drop=True)

    def normalize_text(self):
        self.df["normalized"] = self.df["comment"].apply(self.normalizer.normalize)

    def tokenize_text(self):
        self.df["tokens"] = self.df["normalized"].apply(word_tokenize)

    def eliminate_stopwords(self):
        custom_stopwords = self.stopwords.union({'Ø³Ù„Ø§Ù…', 'Ù…Ù…Ù†ÙˆÙ†', 'Ø®ÙˆØ§Ù‡Ø´', 'Ø§ÙØ²Ø§ÛŒØ´', 'Ú©Ø§Ù‡Ø´'})
        self.df["filtered"] = self.df["tokens"].apply(lambda tokens: [w for w in tokens if w not in custom_stopwords and len(w) > 1])

    def extract_nouns(self):
        def keep_nouns(tokens):
            tagged = self.tagger.tag(tokens)
            nouns = [w for w, tag in tagged if tag.startswith("N") or tag == "Ne"]
            lemmatized = [self.lemmatizer.lemmatize(w).split("#")[0] for w in nouns]
            return lemmatized
        self.df["nouns"] = self.df["filtered"].apply(keep_nouns)

    def tf_idf(self):
        corpus = self.df["nouns"].apply(lambda x: ' '.join(x)).tolist()
        vectorizer = TfidfVectorizer(ngram_range=(2, 5), max_features=1000)
        self.tfidf_matrix = vectorizer.fit_transform(corpus)
        self.feature_names = vectorizer.get_feature_names_out()

    def top_words(self, top_n=40):
        summed = asarray(self.tfidf_matrix.sum(axis=0)).ravel()
        scores = list(zip(self.feature_names, summed))
        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)
        for word, score in sorted_scores[:top_n]:
            print(f"{word}: {score:.4f}")

    def run(self):
        self.read_data()
        self.normalize_text()
        self.tokenize_text()
        self.eliminate_stopwords()
        self.extract_nouns()
        self.tf_idf()
        return self.top_words()


""" TEST """
k = KeyWordExtraction()
k.run()


""" Result simple: """
# ğŸ”‘ Top 20 Keywords:
# Ú©Ø±Ø§ÛŒÙ‡: 8774.2351
# Ø§Ø³Ù†Ù¾: 3542.9436
# Ù‚ÛŒÙ…Øª: 3007.5199
# Ø³ÙØ±: 2870.3018
# Ø·Ø±Ø­: 2525.0969
# Ù…Ø¨Ù„Øº: 2441.0173
# Ø¨Ø§Ú©Ø³: 2010.4909
# Ú©Ù…ÛŒØ³ÛŒÙˆÙ†: 1897.2216
# Ø³Ø±ÙˆÛŒØ³: 1773.9573
# Ù…Ù‚ØµØ¯: 1695.1729
# Ù…Ø³ÛŒØ±: 1457.3849
# Ù‡Ø²ÛŒÙ†Ù‡: 1417.4845
# Ø±Ø§Ù†Ù†Ø¯Ù‡: 1394.6229
# Ø¯Ø±Ø®ÙˆØ§Ø³Øª: 1367.6115
# Ù…ÙˆØªÙˆØ±: 1354.2148
# Ú©Ø§Ø±: 1307.9143
# Ù¾Ø§Ø¯Ø§Ø´: 1170.195
# Ù…Ø¨Ø¯Ø§: 1081.0158
# Ú©Ù…Ù‡: 1064.2538
# Ø¨ÛŒÙ…Ù‡: 1032.0796


""" Result tf-idf"""
# Ú©Ø±Ø§ÛŒÙ‡: 9075.9695
# Ø§Ø³Ù†Ù¾: 3727.2558
# Ù‚ÛŒÙ…Øª: 3060.3144
# Ø³ÙØ±: 2825.6253
# Ø·Ø±Ø­: 2604.1616
# Ù…Ø¨Ù„Øº: 2213.7489
# Ø¨Ø§Ú©Ø³: 2128.5631
# Ø³Ø±ÙˆÛŒØ³: 1877.5064
# Ú©Ù…ÛŒØ³ÛŒÙˆÙ†: 1803.6555
# Ù…Ù‚ØµØ¯: 1693.8258
# Ø±Ø§Ù†Ù†Ø¯Ù‡: 1631.0444
# Ù…Ø¨Ù„Øº Ú©Ø±Ø§ÛŒÙ‡: 1582.1052
# Ù…ÙˆØªÙˆØ±: 1515.0396
# Ø§Ø³Ù†Ù¾ Ø¨Ø§Ú©Ø³: 1509.6169
# Ú©Ø§Ø±: 1506.2837
# Ø¯Ø±Ø®ÙˆØ§Ø³Øª: 1491.2223
# Ù…Ø³ÛŒØ±: 1482.3303
# Ù‡Ø²ÛŒÙ†Ù‡: 1449.8180
# Ù¾Ø§Ø¯Ø§Ø´: 1220.4606
# Ù…Ø´ØªØ±ÛŒ: 1172.5306

""" N -gram"""
# Ù…Ø¨Ù„Øº Ú©Ø±Ø§ÛŒÙ‡: 2379.2240
# Ø§Ø³Ù†Ù¾ Ø¨Ø§Ú©Ø³: 2291.6665
# Ø§Ø³Ù†Ù¾ ÙÙˆØ¯: 905.3307
# Ú©Ø±Ø§ÛŒÙ‡ Ù‡Ø§Ø±Ùˆ: 867.3568
# Ú©Ø±Ø§ÛŒÙ‡ Ø§Ø³Ù†Ù¾: 802.6109
# Ú©Ø±Ø§ÛŒÙ‡ Ú©Ù…Ù‡: 798.3876
# Ú©Ø±Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ†Ù‡: 706.8056
# Ù‚ÛŒÙ…Øª Ú©Ø±Ø§ÛŒÙ‡: 688.4342
# Ø¨ÛŒÙ…Ù‡ ØªØ§Ù…ÛŒÙ†: 618.8005
# Ú©Ø±Ø§ÛŒÙ‡ Ú©Ù…ÛŒØ³ÛŒÙˆÙ†: 597.5375
# Ø§Ø³Ù†Ù¾ Ù…Ø§Ø±Ú©: 593.4369
# ØªØ³ÙˆÛŒÙ‡ Ù„Ø­Ø¸Ù‡: 582.0610
# Ù…Ø¨Ø¯Ø§ Ù…Ù‚ØµØ¯: 579.3380
# Ú©Ø±Ø§ÛŒÙ‡ Ú©Ø±Ø§ÛŒÙ‡: 544.9816
# Ù…Ù‚ØµØ¯ Ù…Ù†ØªØ®Ø¨: 532.3066
# Ù†Ø±Ø® Ú©Ø±Ø§ÛŒÙ‡: 519.0596
# Ù‡Ø²ÛŒÙ†Ù‡ Ø³ÙØ±: 509.2082
# ÙØ§ØµÙ„Ù‡ Ù…Ø¨Ø¯Ø§: 505.1568
# Ù‚ÛŒÙ…Øª Ø³ÙØ±: 497.2048
# Ù„ØºÙˆ Ø³ÙØ±: 482.2130
# Ø³ÙØ± Ø·Ø±Ø­: 474.7676
# Ù…ÙˆØªÙˆØ± Ø³ÙˆØ§Ø±: 455.6272
# Ø§Ø³Ù†Ù¾ Ø´Ø§Ù¾: 446.6013
# Ú©Ø±Ø§ÛŒÙ‡ Ø·Ø±Ø­: 445.3776
# Ú©Ø±Ø§ÛŒÙ‡ Ù…Ø³ÛŒØ±: 442.6498
# Ù…ÙˆØªÙˆØ± Ø³ÛŒÚ©Ù„: 442.1992
# Ø§Ø³Ù†Ù¾ Ø¯Ú©ØªØ±: 410.7954
# Ø³Ø±ÙˆÛŒØ³ Ø§Ø³Ù†Ù¾: 385.1991
# Ø³Ù‡Ù…ÛŒÙ‡ Ø¨Ù†Ø²ÛŒÙ†: 366.8005
# Ø³ÙØ± Ø§Ø³Ù†Ù¾: 350.4944
# Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±: 345.8108
# Ù‡Ø²Ø§Ø± ØªÙˆÙ…Ø§Ù†: 339.3669
# Ø§Ø³Ù†Ù¾ Ú©Ø§Ø±: 312.7900
# Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù‚ØµØ¯: 312.0974
# Ú©Ø±Ø§ÛŒÙ‡ Ø§Ø³Ù†Ù¾ Ø¨Ø§Ú©Ø³: 303.0572
# Ø·Ø±Ø­ Ø³ÙØ±: 301.8259
# Ú©Ø±Ø§ÛŒÙ‡ Ø³ÙØ±: 296.3706
# Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø³Ù†Ù¾: 281.9594
# Û²Û° Ø¯Ø±ØµØ¯: 273.5326
# Ø·Ø±Ø­ ØªØ´ÙˆÛŒÙ‚: 272.7084

# Pricing
# Fuel
# Cancelation
# Incentive
# Commission
# App
# Origen Distance
# Desired Destination
# Insurance
# Instant Cashout
# Other
